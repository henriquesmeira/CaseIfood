# Databricks notebook source
# MAGIC %md
# MAGIC # Case T√©cnico iFood - Data Architect
# MAGIC ## Notebook 1: Extra√ß√£o de Dados (Python)
# MAGIC 
# MAGIC Este notebook implementa a **primeira etapa** da nova arquitetura:
# MAGIC 
# MAGIC ### üèóÔ∏è Arquitetura V2:
# MAGIC 1. **Python**: Extra√ß√£o dos dados (este notebook)
# MAGIC 2. **PySpark**: Consolida√ß√£o no Delta Lake (pr√≥ximo notebook)
# MAGIC 3. **SQL**: Respostas finais do desafio (√∫ltimo notebook)
# MAGIC 
# MAGIC ### üì• Responsabilidades deste Notebook:
# MAGIC - Download dos arquivos Parquet via Python
# MAGIC - Upload para DBFS (Databricks File System)
# MAGIC - Valida√ß√£o dos arquivos baixados

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Configura√ß√£o e Imports

# COMMAND ----------

import requests
import os
import time
from typing import Dict, List, Tuple

# Configura√ß√µes
LOCAL_DATA_DIR = "/tmp/nyc_taxi_data"
DBFS_RAW_DIR = "/tmp/nyc_taxi/raw"  # Usando /tmp que √© permitido

# URLs dos dados NYC Taxi (Janeiro a Maio 2023)
DATA_SOURCES = {
    "yellow": {
        "2023-01": "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet",
        "2023-02": "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet",
        "2023-03": "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet",
        "2023-04": "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-04.parquet",
        "2023-05": "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-05.parquet"
    },
    "green": {
        "2023-01": "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-01.parquet",
        "2023-02": "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-02.parquet",
        "2023-03": "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-03.parquet",
        "2023-04": "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-04.parquet",
        "2023-05": "https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-05.parquet"
    }
}

print("üöÄ EXTRA√á√ÉO DE DADOS NYC TAXI - PYTHON")
print("="*50)
print(f"üìä Total de arquivos: {sum(len(months) for months in DATA_SOURCES.values())}")
print(f"üìÅ Diret√≥rio local: {LOCAL_DATA_DIR}")
print(f"üìÅ Diret√≥rio DBFS: {DBFS_RAW_DIR}")
print(f"üí° Usando /tmp no DBFS (compat√≠vel com Community Edition)")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Fun√ß√µes de Download

# COMMAND ----------

def download_file_python(url: str, local_path: str, max_retries: int = 3) -> bool:
    """
    Download de arquivo via Python requests
    """
    for attempt in range(max_retries):
        try:
            filename = os.path.basename(local_path)
            print(f"  üì• Download tentativa {attempt + 1}: {filename}")
            
            # Headers para simular browser
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            # Download com streaming e timeout
            response = requests.get(
                url, 
                headers=headers, 
                stream=True, 
                timeout=600,  # 10 minutos
                verify=True
            )
            response.raise_for_status()
            
            # Salva arquivo em chunks
            total_size = int(response.headers.get('content-length', 0))
            downloaded_size = 0
            
            with open(local_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded_size += len(chunk)
                        
                        # Progress a cada 50MB
                        if total_size > 0 and downloaded_size % (50 * 1024 * 1024) == 0:
                            progress = (downloaded_size / total_size * 100)
                            print(f"    üìä Progresso: {progress:.1f}%")
            
            # Verifica tamanho final
            file_size = os.path.getsize(local_path)
            if file_size == 0:
                raise ValueError("Arquivo baixado est√° vazio")
            
            print(f"    ‚úÖ Download conclu√≠do: {file_size / (1024*1024):.1f} MB")
            return True
            
        except Exception as e:
            print(f"    ‚ùå Erro: {str(e)[:150]}...")
            
            # Remove arquivo parcial
            try:
                if os.path.exists(local_path):
                    os.remove(local_path)
            except:
                pass
            
            if attempt < max_retries - 1:
                delay = 10 * (2 ** attempt)  # Backoff exponencial
                print(f"    ‚è≥ Aguardando {delay} segundos...")
                time.sleep(delay)
            else:
                print(f"    üí• Falha definitiva ap√≥s {max_retries} tentativas")
                return False

def upload_to_dbfs(local_path: str, dbfs_path: str) -> bool:
    """
    Upload do arquivo local para DBFS (com fallback para local)
    """
    try:
        filename = os.path.basename(local_path)
        print(f"  üì§ Upload para DBFS: {filename}")

        # Verifica se arquivo local existe
        if not os.path.exists(local_path):
            raise FileNotFoundError(f"Arquivo local n√£o encontrado: {local_path}")

        # Cria diret√≥rio DBFS se n√£o existir
        try:
            dbutils.fs.mkdirs(os.path.dirname(dbfs_path))
        except Exception as mkdir_error:
            print(f"    ‚ö†Ô∏è  Erro ao criar diret√≥rio DBFS: {mkdir_error}")
            if "Public DBFS root is disabled" in str(mkdir_error):
                print(f"    üí° DBFS p√∫blico desabilitado - mantendo arquivo local")
                return True  # Considera sucesso, mas mant√©m local
            raise mkdir_error

        # Remove arquivo DBFS se existir
        try:
            dbutils.fs.rm(dbfs_path)
        except:
            pass  # Arquivo pode n√£o existir

        # Copia arquivo
        dbutils.fs.cp(f"file:{local_path}", dbfs_path)

        # Verifica se upload foi bem-sucedido
        file_info = dbutils.fs.ls(dbfs_path)
        if file_info:
            dbfs_size = file_info[0].size
            local_size = os.path.getsize(local_path)

            if dbfs_size != local_size:
                raise ValueError(f"Tamanhos diferentes: local={local_size}, dbfs={dbfs_size}")

            print(f"    ‚úÖ Upload conclu√≠do: {dbfs_size / (1024*1024):.1f} MB")
            return True
        else:
            raise ValueError("Arquivo n√£o encontrado no DBFS ap√≥s upload")

    except Exception as e:
        print(f"    ‚ùå Erro no upload: {e}")
        if "Public DBFS root is disabled" in str(e):
            print(f"    üí° Mantendo arquivo local: {local_path}")
            return True  # Considera sucesso para continuar processamento
        return False

print("‚úÖ Fun√ß√µes de download definidas!")

# COMMAND ----------

# MAGIC %md
# MAGIC ### üí° Troubleshooting DBFS
# MAGIC
# MAGIC **Se voc√™ receber erro "Public DBFS root is disabled":**
# MAGIC
# MAGIC 1. **Problema**: Databricks Community Edition desabilitou `/FileStore/`
# MAGIC 2. **Solu√ß√£o**: Usamos `/tmp/` que √© permitido
# MAGIC 3. **Alternativa**: Use apenas armazenamento local se DBFS falhar completamente

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Prepara√ß√£o do Ambiente

# COMMAND ----------

# Cria diret√≥rio local
os.makedirs(LOCAL_DATA_DIR, exist_ok=True)
print(f"üìÅ Diret√≥rio local criado: {LOCAL_DATA_DIR}")

# Limpa diret√≥rio DBFS se existir
try:
    dbutils.fs.rm(DBFS_RAW_DIR, True)
    print(f"üóëÔ∏è  Diret√≥rio DBFS limpo: {DBFS_RAW_DIR}")
except:
    print(f"üìÅ Diret√≥rio DBFS ser√° criado: {DBFS_RAW_DIR}")

# Cria diret√≥rio DBFS (usando /tmp que √© permitido)
try:
    dbutils.fs.mkdirs(DBFS_RAW_DIR)
    print(f"‚úÖ Diret√≥rio DBFS criado: {DBFS_RAW_DIR}")
except Exception as e:
    print(f"‚ö†Ô∏è  Erro ao criar diret√≥rio DBFS: {e}")
    print(f"üí° Tentando diret√≥rio alternativo...")

    # Fallback para diret√≥rio do usu√°rio
    alternative_dir = f"/tmp/nyc_taxi_raw_{spark.sparkContext.sparkUser()}"
    try:
        dbutils.fs.mkdirs(alternative_dir)
        DBFS_RAW_DIR = alternative_dir
        print(f"‚úÖ Diret√≥rio alternativo criado: {DBFS_RAW_DIR}")
    except Exception as e2:
        print(f"‚ùå Falha total: {e2}")
        print(f"üí° Use dados locais apenas ou configure workspace com DBFS habilitado")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Extra√ß√£o Principal

# COMMAND ----------

# Contadores
downloaded_files = []
failed_downloads = []

print("üöÄ INICIANDO EXTRA√á√ÉO DOS DADOS")
print("="*40)

for taxi_type, months in DATA_SOURCES.items():
    print(f"\nüöï Processando {taxi_type}:")
    
    for year_month, url in months.items():
        filename = f"{taxi_type}_tripdata_{year_month}.parquet"
        local_path = os.path.join(LOCAL_DATA_DIR, filename)
        dbfs_path = f"{DBFS_RAW_DIR}/{filename}"
        
        # Download
        if download_file_python(url, local_path):
            # Upload para DBFS (ou mant√©m local se DBFS falhar)
            upload_success = upload_to_dbfs(local_path, dbfs_path)

            if upload_success:
                # Verifica se arquivo realmente existe no DBFS
                try:
                    dbutils.fs.ls(dbfs_path)
                    actual_dbfs_path = dbfs_path
                    can_remove_local = True
                except:
                    # DBFS falhou, usa arquivo local
                    actual_dbfs_path = f"file:{local_path}"
                    can_remove_local = False
                    print(f"    üí° Usando arquivo local: {local_path}")

                downloaded_files.append({
                    'taxi_type': taxi_type,
                    'year_month': year_month,
                    'filename': filename,
                    'local_path': local_path,
                    'dbfs_path': actual_dbfs_path,
                    'url': url,
                    'is_local_only': not can_remove_local
                })

                # Remove arquivo local apenas se upload DBFS foi bem-sucedido
                if can_remove_local:
                    try:
                        os.remove(local_path)
                        print(f"    üóëÔ∏è  Arquivo local removido")
                    except:
                        pass
                else:
                    print(f"    üìÅ Arquivo mantido localmente")
            else:
                failed_downloads.append({
                    'filename': filename,
                    'reason': 'Upload para DBFS falhou'
                })
        else:
            failed_downloads.append({
                'filename': filename,
                'reason': 'Download falhou'
            })

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Relat√≥rio de Extra√ß√£o

# COMMAND ----------

print("üìä RELAT√ìRIO DE EXTRA√á√ÉO")
print("="*40)
print(f"   ‚Ä¢ Sucessos: {len(downloaded_files)}")
print(f"   ‚Ä¢ Falhas: {len(failed_downloads)}")

total_files = len(downloaded_files) + len(failed_downloads)
if total_files > 0:
    success_rate = (len(downloaded_files) / total_files) * 100
    print(f"   ‚Ä¢ Taxa de sucesso: {success_rate:.1f}%")

if downloaded_files:
    print(f"\n‚úÖ Arquivos extra√≠dos com sucesso:")
    for file_info in downloaded_files:
        print(f"   ‚Ä¢ {file_info['filename']} ‚Üí {file_info['dbfs_path']}")

if failed_downloads:
    print(f"\n‚ùå Arquivos com falha:")
    for file_info in failed_downloads:
        print(f"   ‚Ä¢ {file_info['filename']}: {file_info['reason']}")

print(f"\nüìÅ Arquivos dispon√≠veis no DBFS: {DBFS_RAW_DIR}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 6. Valida√ß√£o dos Arquivos

# COMMAND ----------

print("üîç VALIDA√á√ÉO DOS ARQUIVOS EXTRA√çDOS")
print("="*40)

try:
    # Lista arquivos no DBFS
    dbfs_files = dbutils.fs.ls(DBFS_RAW_DIR)
    
    print(f"üìÇ Arquivos encontrados no DBFS ({len(dbfs_files)}):")
    total_size = 0
    
    for file_info in dbfs_files:
        file_size_mb = file_info.size / (1024 * 1024)
        total_size += file_size_mb
        print(f"   ‚Ä¢ {file_info.name}: {file_size_mb:.1f} MB")
    
    print(f"\nüìä Tamanho total: {total_size:.1f} MB")
    
    # Verifica se todos os arquivos esperados est√£o presentes
    expected_files = []
    for taxi_type, months in DATA_SOURCES.items():
        for year_month in months.keys():
            expected_files.append(f"{taxi_type}_tripdata_{year_month}.parquet")
    
    present_files = [f.name for f in dbfs_files]
    missing_files = [f for f in expected_files if f not in present_files]
    
    if missing_files:
        print(f"\n‚ö†Ô∏è  Arquivos ausentes ({len(missing_files)}):")
        for filename in missing_files:
            print(f"   ‚Ä¢ {filename}")
    else:
        print(f"\n‚úÖ Todos os arquivos esperados est√£o presentes!")
    
except Exception as e:
    print(f"‚ùå Erro na valida√ß√£o: {e}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 7. Prepara√ß√£o para Pr√≥xima Etapa

# COMMAND ----------

# Salva informa√ß√µes dos arquivos para pr√≥ximo notebook
if downloaded_files:
    print("üìã PREPARANDO PARA CONSOLIDA√á√ÉO PYSPARK")
    print("="*40)
    
    # Cria lista de arquivos para o pr√≥ximo notebook
    files_for_consolidation = []
    
    for file_info in downloaded_files:
        files_for_consolidation.append({
            'taxi_type': file_info['taxi_type'],
            'year_month': file_info['year_month'],
            'dbfs_path': file_info['dbfs_path'],
            'filename': file_info['filename']
        })
    
    print(f"‚úÖ {len(files_for_consolidation)} arquivos prontos para consolida√ß√£o")
    print(f"üìÅ Localiza√ß√£o: {DBFS_RAW_DIR}")
    
    # Mostra exemplo de como acessar no pr√≥ximo notebook
    print(f"\nüí° Para usar no pr√≥ximo notebook:")

    # Verifica se h√° arquivos locais vs DBFS
    local_files = [f for f in files_for_consolidation if f.get('is_local_only', False)]
    dbfs_files = [f for f in files_for_consolidation if not f.get('is_local_only', False)]

    if dbfs_files:
        print(f"```python")
        print(f"# Listar arquivos no DBFS")
        print(f"dbutils.fs.ls('{DBFS_RAW_DIR}')")
        print(f"")
        print(f"# Ler arquivo do DBFS")
        print(f"df = spark.read.parquet('{DBFS_RAW_DIR}/yellow_tripdata_2023-01.parquet')")
        print(f"```")

    if local_files:
        print(f"\n‚ö†Ô∏è  Alguns arquivos est√£o apenas localmente:")
        print(f"```python")
        print(f"# Ler arquivo local")
        print(f"df = spark.read.parquet('file:/tmp/nyc_taxi_data/yellow_tripdata_2023-01.parquet')")
        print(f"```")
    
else:
    print("‚ùå Nenhum arquivo foi extra√≠do com sucesso!")
    print("üí° Verifique a conectividade e tente novamente")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 8. Limpeza e Finaliza√ß√£o

# COMMAND ----------

# Limpa diret√≥rio local tempor√°rio
try:
    import shutil
    if os.path.exists(LOCAL_DATA_DIR):
        shutil.rmtree(LOCAL_DATA_DIR)
        print(f"üóëÔ∏è  Diret√≥rio local removido: {LOCAL_DATA_DIR}")
except Exception as e:
    print(f"‚ö†Ô∏è  Erro ao limpar diret√≥rio local: {e}")

print(f"\nüéâ EXTRA√á√ÉO CONCLU√çDA!")
print("="*30)
print(f"‚úÖ Arquivos extra√≠dos: {len(downloaded_files)}")
print(f"‚ùå Arquivos com falha: {len(failed_downloads)}")

# Mostra onde os dados est√£o
local_only = sum(1 for f in downloaded_files if f.get('is_local_only', False))
dbfs_files = len(downloaded_files) - local_only

if dbfs_files > 0:
    print(f"üìÅ Arquivos no DBFS: {dbfs_files} em {DBFS_RAW_DIR}")
if local_only > 0:
    print(f"üìÅ Arquivos locais: {local_only} em {LOCAL_DATA_DIR}")

print(f"\nüöÄ Pr√≥ximo passo: Execute o notebook de Consolida√ß√£o PySpark")
print(f"üí° O pr√≥ximo notebook detectar√° automaticamente a localiza√ß√£o dos arquivos")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Resumo da Extra√ß√£o
# MAGIC 
# MAGIC ### ‚úÖ Etapa 1 Conclu√≠da: Extra√ß√£o Python
# MAGIC 
# MAGIC **O que foi feito:**
# MAGIC - ‚úÖ Download de arquivos Parquet via Python requests
# MAGIC - ‚úÖ Upload para DBFS (Databricks File System)
# MAGIC - ‚úÖ Valida√ß√£o dos arquivos extra√≠dos
# MAGIC - ‚úÖ Prepara√ß√£o para pr√≥xima etapa
# MAGIC 
# MAGIC **Arquivos processados:**
# MAGIC - Yellow Taxis: Janeiro a Maio 2023
# MAGIC - Green Taxis: Janeiro a Maio 2023
# MAGIC - Total: 10 arquivos Parquet
# MAGIC 
# MAGIC **Localiza√ß√£o dos dados:**
# MAGIC - DBFS: `/FileStore/nyc_taxi/raw/`
# MAGIC - Formato: Parquet (dados brutos)
# MAGIC 
# MAGIC ### üöÄ Pr√≥ximos Passos:
# MAGIC 1. **Notebook 2**: Consolida√ß√£o PySpark (Raw ‚Üí Bronze ‚Üí Silver ‚Üí Gold)
# MAGIC 2. **Notebook 3**: An√°lises SQL (Respostas √†s perguntas do case)
# MAGIC 
# MAGIC ### üèóÔ∏è Arquitetura Implementada:
# MAGIC ```
# MAGIC [Python Download] ‚Üí [DBFS Storage] ‚Üí [PySpark Processing] ‚Üí [SQL Analysis]
# MAGIC      ‚úÖ FEITO           ‚úÖ FEITO         üîÑ PR√ìXIMO         üìä FINAL
# MAGIC ```
