# Detalhes TÃ©cnicos - Case iFood Data Architect

## ğŸ—ï¸ Arquitetura da SoluÃ§Ã£o

### VisÃ£o Geral
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Sources  â”‚    â”‚   Processing     â”‚    â”‚   Data Lake     â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â€¢ NYC Taxi APIs â”‚â”€â”€â”€â–¶â”‚ â€¢ PySpark ETL    â”‚â”€â”€â”€â–¶â”‚ â€¢ Delta Lake    â”‚
â”‚ â€¢ Parquet Files â”‚    â”‚ â€¢ Data Quality   â”‚    â”‚ â€¢ Partitioned   â”‚
â”‚ â€¢ CloudFront    â”‚    â”‚ â€¢ Standardizationâ”‚    â”‚ â€¢ Optimized     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚    Analytics     â”‚
                       â”‚                  â”‚
                       â”‚ â€¢ Business Q&A   â”‚
                       â”‚ â€¢ Exploratory    â”‚
                       â”‚ â€¢ Visualizations â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes Principais

#### 1. **Camada de IngestÃ£o** (`src/data_ingestion.py`)
- **Responsabilidade**: Download e processamento inicial
- **Tecnologias**: PySpark, HTTP requests
- **Features**:
  - Retry automÃ¡tico com backoff exponencial
  - PadronizaÃ§Ã£o de schemas entre Yellow/Green taxis
  - ValidaÃ§Ã£o de dados em tempo real
  - Processamento distribuÃ­do

#### 2. **Camada de Qualidade** (`src/data_quality.py`)
- **Responsabilidade**: ValidaÃ§Ã£o e monitoramento
- **Features**:
  - VerificaÃ§Ã£o de colunas obrigatÃ³rias
  - AnÃ¡lise de valores nulos e outliers
  - ValidaÃ§Ã£o de ranges de dados
  - RelatÃ³rios de qualidade automatizados

#### 3. **Camada de Armazenamento** (Delta Lake)
- **Tecnologia**: Delta Lake sobre Databricks
- **CaracterÃ­sticas**:
  - ACID transactions
  - Schema evolution
  - Time travel
  - OtimizaÃ§Ãµes automÃ¡ticas (Z-ORDER, OPTIMIZE)

#### 4. **Camada de AnÃ¡lise** (`analysis/`)
- **Responsabilidade**: Business intelligence e insights
- **Componentes**:
  - Respostas Ã s perguntas de negÃ³cio
  - AnÃ¡lise exploratÃ³ria
  - VisualizaÃ§Ãµes e relatÃ³rios

## ğŸ”§ DecisÃµes TÃ©cnicas

### Escolha do Delta Lake
**Justificativa**:
- âœ… ACID compliance para consistÃªncia
- âœ… Schema evolution para flexibilidade
- âœ… Performance otimizada para analytics
- âœ… IntegraÃ§Ã£o nativa com Databricks
- âœ… Suporte a time travel para auditoria

### Particionamento EstratÃ©gico
**EstratÃ©gia**: `taxi_type`, `year`, `month`
**Justificativa**:
- Consultas frequentes por tipo de tÃ¡xi
- AnÃ¡lises temporais (mensal/anual)
- OtimizaÃ§Ã£o de pruning de partiÃ§Ãµes
- ParalelizaÃ§Ã£o eficiente

### PadronizaÃ§Ã£o de Schema
**Problema**: Green taxis usam `lpep_*` vs Yellow `tpep_*`
**SoluÃ§Ã£o**: RenomeaÃ§Ã£o automÃ¡tica para `tpep_*`
**BenefÃ­cio**: Schema unificado para anÃ¡lises

### Tratamento de Erros
**EstratÃ©gia**: Retry com backoff + logging detalhado
**ImplementaÃ§Ã£o**:
```python
for attempt in range(max_retries):
    try:
        # Processamento
        return success
    except Exception as e:
        if attempt < max_retries - 1:
            time.sleep(delay * (2 ** attempt))
        else:
            log_error_and_continue()
```

## ğŸ“Š Modelo de Dados

### Schema Final da Tabela
```sql
CREATE TABLE main.nyc_taxi.trips_delta (
    -- Colunas obrigatÃ³rias (conforme especificaÃ§Ã£o)
    VendorID BIGINT,
    passenger_count DOUBLE,
    total_amount DOUBLE,
    tpep_pickup_datetime TIMESTAMP,
    tpep_dropoff_datetime TIMESTAMP,
    
    -- Colunas adicionais Ãºteis
    trip_distance DOUBLE,
    fare_amount DOUBLE,
    tip_amount DOUBLE,
    tolls_amount DOUBLE,
    
    -- Metadados para particionamento e anÃ¡lise
    taxi_type STRING,
    year INT,
    month INT,
    year_month STRING,
    pickup_hour INT
)
USING DELTA
PARTITIONED BY (taxi_type, year, month)
```

### Qualidade dos Dados
**MÃ©tricas Monitoradas**:
- Taxa de valores nulos por coluna
- Outliers em tarifas e passageiros
- ConsistÃªncia temporal (pickup < dropoff)
- Ranges vÃ¡lidos para campos numÃ©ricos

## âš¡ OtimizaÃ§Ãµes Implementadas

### 1. **Spark Configurations**
```python
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.databricks.delta.schema.autoMerge.enabled", "true")
spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "true")
```

### 2. **Delta Lake Optimizations**
```sql
-- CompactaÃ§Ã£o de arquivos pequenos
OPTIMIZE main.nyc_taxi.trips_delta

-- Z-ORDER para consultas por datetime e tipo
OPTIMIZE main.nyc_taxi.trips_delta 
ZORDER BY (tpep_pickup_datetime, taxi_type)

-- EstatÃ­sticas para otimizador
ANALYZE TABLE main.nyc_taxi.trips_delta COMPUTE STATISTICS
```

### 3. **Processamento Eficiente**
- Cache estratÃ©gico de DataFrames intermediÃ¡rios
- Union otimizado para combinaÃ§Ã£o de datasets
- SeleÃ§Ã£o de colunas antes de joins
- Particionamento alinhado com padrÃµes de consulta

## ğŸ” AnÃ¡lises Implementadas

### Pergunta 1: MÃ©dia Yellow Taxis
**Query Otimizada**:
```sql
SELECT 
    year_month,
    AVG(total_amount) as avg_fare,
    COUNT(*) as trips
FROM main.nyc_taxi.trips_delta 
WHERE taxi_type = 'yellow' 
  AND total_amount > 0
GROUP BY year_month
ORDER BY year_month
```

**OtimizaÃ§Ãµes**:
- Filtro por partiÃ§Ã£o (`taxi_type`)
- Predicate pushdown (`total_amount > 0`)
- AgregaÃ§Ã£o distribuÃ­da

### Pergunta 2: Passageiros por Hora (Maio)
**Query Otimizada**:
```sql
SELECT 
    HOUR(tpep_pickup_datetime) as hour,
    AVG(passenger_count) as avg_passengers,
    COUNT(*) as trips
FROM main.nyc_taxi.trips_delta 
WHERE year_month = '2023-05'
  AND passenger_count > 0
GROUP BY HOUR(tpep_pickup_datetime)
ORDER BY hour
```

**OtimizaÃ§Ãµes**:
- Filtro por partiÃ§Ã£o (`year_month`)
- FunÃ§Ã£o nativa `HOUR()` para extraÃ§Ã£o
- AgregaÃ§Ã£o por hora (24 grupos apenas)

## ğŸš€ Performance e Escalabilidade

### MÃ©tricas de Performance
- **IngestÃ£o**: ~15-20 minutos para 5 meses de dados
- **Consultas**: Sub-segundo para agregaÃ§Ãµes simples
- **Armazenamento**: ~60-70% de compressÃ£o vs CSV

### Escalabilidade Horizontal
- Processamento distribuÃ­do via Spark
- Particionamento para paralelizaÃ§Ã£o
- Auto-scaling do cluster Databricks

### Monitoramento
- Logs detalhados em cada etapa
- Contadores de sucesso/falha
- MÃ©tricas de qualidade automatizadas
- RelatÃ³rios de performance

## ğŸ”’ ConsideraÃ§Ãµes de SeguranÃ§a

### Acesso aos Dados
- Unity Catalog para governanÃ§a
- Controle de acesso baseado em roles
- Auditoria de acessos

### Qualidade e Integridade
- ValidaÃ§Ãµes automÃ¡ticas
- Checksums para integridade
- Versionamento via Delta Lake

## ğŸ“ˆ PrÃ³ximas Melhorias

### Curto Prazo
- [ ] Implementar streaming para dados em tempo real
- [ ] Adicionar mais validaÃ§Ãµes de qualidade
- [ ] Criar dashboards interativos

### MÃ©dio Prazo
- [ ] Machine Learning para detecÃ§Ã£o de anomalias
- [ ] AnÃ¡lises preditivas de demanda
- [ ] IntegraÃ§Ã£o com ferramentas de BI

### Longo Prazo
- [ ] Data mesh architecture
- [ ] Real-time analytics
- [ ] Advanced ML pipelines

## ğŸ› ï¸ Ferramentas e Tecnologias

### Core Stack
- **PySpark**: Processamento distribuÃ­do
- **Delta Lake**: Storage layer
- **Databricks**: Plataforma de execuÃ§Ã£o
- **Python**: Linguagem principal

### Bibliotecas Utilizadas
- `pyspark.sql`: DataFrames e SQL
- `pyspark.sql.functions`: FunÃ§Ãµes built-in
- `time`: Controle de retry
- `typing`: Type hints para qualidade

### Ambiente de Desenvolvimento
- **Databricks Community Edition**: Gratuito
- **Runtime 13.3 LTS**: EstÃ¡vel e otimizado
- **Auto-scaling clusters**: Performance adaptativa

## ğŸ“ DocumentaÃ§Ã£o e Manutenibilidade

### CÃ³digo Documentado
- Docstrings em todas as funÃ§Ãµes
- ComentÃ¡rios explicativos
- Type hints para clareza

### Estrutura Modular
- SeparaÃ§Ã£o de responsabilidades
- ConfiguraÃ§Ãµes centralizadas
- ReutilizaÃ§Ã£o de componentes

### Testes e ValidaÃ§Ã£o
- ValidaÃ§Ãµes automÃ¡ticas de dados
- Logs detalhados para debugging
- RelatÃ³rios de qualidade
