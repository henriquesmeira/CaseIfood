# Estrutura do Projeto - Case TÃ©cnico iFood

## ğŸ“ VisÃ£o Geral da Estrutura

```
ifood-case/
â”œâ”€â”€ ğŸ“‹ README.md                         # DocumentaÃ§Ã£o principal do projeto
â”œâ”€â”€ ğŸš€ EXECUTION_GUIDE.md                # Guia passo-a-passo de execuÃ§Ã£o
â”œâ”€â”€ ğŸ”§ TECHNICAL_DETAILS.md              # Detalhes tÃ©cnicos e arquitetura
â”œâ”€â”€ ğŸ“Š PROJECT_STRUCTURE.md              # Este arquivo - estrutura do projeto
â”œâ”€â”€ ğŸ“¦ requirements.txt                  # DependÃªncias Python
â”‚
â”œâ”€â”€ ğŸ“‚ src/                              # CÃ³digo fonte V2
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                        # ConfiguraÃ§Ãµes das 4 camadas
â”‚   â”œâ”€â”€ data_extraction.py               # ExtraÃ§Ã£o via Python
â”‚   â”œâ”€â”€ data_consolidation.py            # ConsolidaÃ§Ã£o PySpark (4 camadas)
â”‚   â””â”€â”€ main_pipeline_v2.py              # Orquestrador completo V2
â”‚
â”œâ”€â”€ ğŸ“‚ sql/                              # Consultas SQL
â”‚   â””â”€â”€ business_questions.sql           # Respostas finais em SQL
â”‚
â””â”€â”€ ğŸ“‚ notebooks/                        # Notebooks V2 (ExecuÃ§Ã£o Sequencial)
    â”œâ”€â”€ 01_Data_Extraction_Python.py     # ExtraÃ§Ã£o Python
    â”œâ”€â”€ 02_Data_Consolidation_PySpark.py # ConsolidaÃ§Ã£o PySpark
    â””â”€â”€ 03_Business_Analysis_SQL.py      # AnÃ¡lises SQL
```

## ğŸ“‹ DescriÃ§Ã£o dos Arquivos

### ğŸ“„ DocumentaÃ§Ã£o

| Arquivo | DescriÃ§Ã£o | PÃºblico-Alvo |
|---------|-----------|--------------|
| `README.md` | VisÃ£o geral do projeto, objetivos e instruÃ§Ãµes bÃ¡sicas | Todos |
| `EXECUTION_GUIDE.md` | Guia detalhado de execuÃ§Ã£o passo-a-passo | Executores |
| `TECHNICAL_DETAILS.md` | Arquitetura, decisÃµes tÃ©cnicas e otimizaÃ§Ãµes | Desenvolvedores |
| `PROJECT_STRUCTURE.md` | Estrutura e organizaÃ§Ã£o do projeto | Desenvolvedores |

### âš¡ ExecuÃ§Ã£o RÃ¡pida

| Arquivo | DescriÃ§Ã£o | Uso |
|---------|-----------|-----|
| `quick_start.py` | Script completo para execuÃ§Ã£o rÃ¡pida no Databricks | DemonstraÃ§Ã£o |
| `requirements.txt` | DependÃªncias Python para desenvolvimento local | Setup |

### ğŸ”§ CÃ³digo Fonte V2 (`src/`)

| Arquivo | Responsabilidade | Principais Classes/FunÃ§Ãµes |
|---------|------------------|---------------------------|
| `config.py` | ConfiguraÃ§Ãµes das 4 camadas | `get_table_name()`, camadas Raw/Bronze/Silver/Gold |
| `data_extraction.py` | ExtraÃ§Ã£o via Python | `NYCTaxiDataExtractor` |
| `data_consolidation.py` | ConsolidaÃ§Ã£o PySpark | `DataLakeConsolidator` |
| `main_pipeline_v2.py` | OrquestraÃ§Ã£o completa V2 | `NYCTaxiPipelineV2` |

### ğŸ“Š Consultas SQL (`sql/`)

| Arquivo | Foco | SaÃ­das |
|---------|------|--------|
| `business_questions.sql` | Perguntas do case + anÃ¡lises | Respostas SQL otimizadas |

### ğŸ““ Notebooks V2 (`notebooks/`)

| Notebook | Objetivo | Tempo Estimado |
|----------|----------|----------------|
| `01_Data_Extraction_Python.py` | ExtraÃ§Ã£o via Python | 10-15 min |
| `02_Data_Consolidation_PySpark.py` | ConsolidaÃ§Ã£o 4 camadas | 15-20 min |
| `03_Business_Analysis_SQL.py` | AnÃ¡lises SQL finais | 5-10 min |

## ğŸ”„ Fluxo de ExecuÃ§Ã£o

### OpÃ§Ã£o 1: Notebooks V2 (Recomendado)
```
1. 01_Data_Extraction_Python.py
   â”œâ”€â”€ Download via Python requests
   â”œâ”€â”€ Upload para DBFS
   â”œâ”€â”€ ValidaÃ§Ã£o dos arquivos
   â””â”€â”€ PreparaÃ§Ã£o para PySpark

2. 02_Data_Consolidation_PySpark.py
   â”œâ”€â”€ Camada Raw (dados brutos)
   â”œâ”€â”€ Camada Bronze (padronizados)
   â”œâ”€â”€ Camada Silver (enriquecidos)
   â”œâ”€â”€ Camada Gold (agregados)
   â””â”€â”€ OtimizaÃ§Ã£o Delta Lake

3. 03_Business_Analysis_SQL.py
   â”œâ”€â”€ Pergunta 1: MÃ©dia Yellow Taxis
   â”œâ”€â”€ Pergunta 2: Passageiros por hora (Maio)
   â”œâ”€â”€ AnÃ¡lises complementares
   â””â”€â”€ Insights de negÃ³cio
```

### OpÃ§Ã£o 2: Pipeline Completo V2
```
main_pipeline_v2.py
â”œâ”€â”€ Executa data_extraction.py (Python)
â”œâ”€â”€ Executa data_consolidation.py (PySpark)
â”œâ”€â”€ Executa business_questions.sql (SQL)
â””â”€â”€ Gera relatÃ³rios finais
```

## ğŸ¯ Pontos de Entrada

### Para Avaliadores do Case
1. **ExecuÃ§Ã£o Sequencial**: Execute notebooks V2 na ordem (01 â†’ 02 â†’ 03)
2. **Pipeline Completo**: Execute `main_pipeline_v2.py`
3. **CÃ³digo Fonte**: Revise arquivos em `src/` (versÃ£o V2)

### Para Desenvolvedores
1. **ConfiguraÃ§Ãµes**: Comece com `src/config.py` (4 camadas)
2. **Pipeline Principal**: `src/main_pipeline_v2.py`
3. **Consultas**: Adicione anÃ¡lises em `sql/`

### Para UsuÃ¡rios Finais
1. **Resultados**: Notebook `03_Business_Analysis_SQL.py`
2. **Consultas**: Arquivo `sql/business_questions.sql`
3. **Dados**: Tabela `main.nyc_taxi.gold_trips` (camada analÃ­tica)

## ğŸ“Š SaÃ­das do Projeto

### Dados Processados (4 Camadas)
- **Raw**: `main.nyc_taxi.raw_trips` (dados brutos)
- **Bronze**: `main.nyc_taxi.bronze_trips` (padronizados)
- **Silver**: `main.nyc_taxi.silver_trips` (enriquecidos)
- **Gold**: `main.nyc_taxi.gold_trips` (agregados para anÃ¡lises)

### AnÃ¡lises Geradas
- **Pergunta 1**: MÃ©dia de tarifas Yellow Taxis (SQL otimizado)
- **Pergunta 2**: MÃ©dia de passageiros por hora em Maio (SQL otimizado)
- **Complementares**: Insights e anÃ¡lises adicionais

### Artefatos TÃ©cnicos V2
- **ExtraÃ§Ã£o Python**: Download robusto com retry
- **ConsolidaÃ§Ã£o PySpark**: 4 camadas Delta Lake
- **AnÃ¡lises SQL**: Consultas otimizadas na camada Gold
- **DocumentaÃ§Ã£o**: Guias atualizados para V2

## ğŸ”§ ConfiguraÃ§Ãµes Importantes

### Spark Configurations
```python
# Em config.py
SPARK_CONFIGS = {
    "spark.sql.adaptive.enabled": "true",
    "spark.databricks.delta.schema.autoMerge.enabled": "true",
    # ... outras configuraÃ§Ãµes
}
```

### Fontes de Dados
```python
# URLs dos dados NYC Taxi
DATA_SOURCES = {
    "yellow": {"2023-01": "https://...", ...},
    "green": {"2023-01": "https://...", ...}
}
```

### Colunas ObrigatÃ³rias
```python
REQUIRED_COLUMNS = [
    "VendorID", "passenger_count", "total_amount",
    "tpep_pickup_datetime", "tpep_dropoff_datetime"
]
```

## ğŸš€ Como ComeÃ§ar

### ExecuÃ§Ã£o V2 Sequencial (30-40 minutos)
1. Abra o Databricks Community Edition
2. Importe os 3 notebooks V2
3. Execute na ordem: 01 â†’ 02 â†’ 03
4. Veja resultados SQL finais

### ExecuÃ§Ã£o V2 Pipeline Completo (35-45 minutos)
1. Clone o repositÃ³rio
2. Importe `src/main_pipeline_v2.py`
3. Execute o pipeline completo
4. Explore tabelas Delta criadas

### Desenvolvimento Local
1. `pip install -r requirements.txt`
2. Configure ambiente Spark local
3. Adapte configuraÃ§Ãµes em `config.py` (4 camadas)
4. Execute mÃ³dulos V2 separadamente

## ğŸ“ Suporte

- **Issues**: Problemas tÃ©cnicos ou dÃºvidas
- **DocumentaÃ§Ã£o**: Consulte os arquivos `.md`
- **CÃ³digo**: ComentÃ¡rios detalhados nos scripts
- **Logs**: SaÃ­das detalhadas durante execuÃ§Ã£o
