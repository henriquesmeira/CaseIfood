# Case TÃ©cnico Data Architect - iFood

## Objetivo

Este desafio permitirÃ¡ que vocÃª demonstre suas habilidades em Engenharia de Dados/Software, AnÃ¡lise e Modelagem de Dados.

Neste case tÃ©cnico, vocÃª deverÃ¡ fazer a ingestÃ£o de alguns dados em nosso Data Lake e pensar em uma forma de disponibilizÃ¡-los para os consumidores. Para finalizar, vocÃª deverÃ¡ realizar anÃ¡lises sobre os dados disponibilizados.

## Dados DisponÃ­veis

Os dados estÃ£o disponÃ­veis no site da agÃªncia responsÃ¡vel por licenciar e regular os tÃ¡xis na cidade de NY: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page

Num primeiro momento, precisamos que sejam armazenados e disponibilizados os dados de Janeiro a Maio de 2023.

## Estrutura do RepositÃ³rio

```
ifood-case/
â”œâ”€â”€ src/                    # CÃ³digo fonte da soluÃ§Ã£o
â”œâ”€â”€ analysis/              # Scripts/Notebooks com as respostas das perguntas
â”œâ”€â”€ README.md              # Este arquivo
â””â”€â”€ requirements.txt       # DependÃªncias do projeto
```

## ConsideraÃ§Ãµes

- VocÃª pode considerar inicialmente armazenar todos os arquivos originais em uma landing zone (que pode ser um bucket do S3, por exemplo, ou qualquer outra tecnologia de sua escolha);

- VocÃª pode considerar manipular/limpar os dados que julgar necessÃ¡rio;

- VocÃª precisa garantir que as colunas **VendorID**, **passenger_count**, **total_amount**, **tpep_pickup_datetime** e **tpep_dropoff_datetime** estejam presentes na camada de consumo. As outras colunas podem ser ignoradas;

- VocÃª pode considerar que no Data Lake nÃ£o existe nenhuma tabela criada, portanto, precisam ser modeladas e criadas.

## O Desafio

VocÃª deverÃ¡ entregar:

1. **SoluÃ§Ã£o para ler os dados originais, fazer a ingestÃ£o no Data Lake e disponibilizar aos usuÃ¡rios finais**
   - Deve utilizar PySpark em alguma etapa;
   - Recomendamos usar Databricks Community Edition (https://community.cloud.databricks.com/);
   - A escolha da tecnologia de metadados fica a seu critÃ©rio;
   - A escolha da linguagem de consulta (SQL, PySpark e etc) para os usuÃ¡rios finais fica a seu critÃ©rio;

2. **CÃ³digo SQL ou PySpark estruturado da forma que preferir com as respostas para as seguintes perguntas:**
   - Qual a mÃ©dia de valor total (total_amount) recebido em um mÃªs considerando todos os yellow taxis da frota?
   - Qual a mÃ©dia de passageiros (passenger_count) por cada hora do dia que pegaram tÃ¡xi no mÃªs de maio considerando todos os tÃ¡xis da frota?

## CritÃ©rios de AvaliaÃ§Ã£o

SerÃ£o avaliados:
- Qualidade e organizaÃ§Ã£o do cÃ³digo
- Processo de anÃ¡lise exploratÃ³ria
- Justificativa das escolhas tÃ©cnicas
- Criatividade na soluÃ§Ã£o proposta
- Clareza na comunicaÃ§Ã£o dos resultados

## InstruÃ§Ãµes de ExecuÃ§Ã£o

1. Crie um repositÃ³rio pÃºblico ou privado no GitHub
2. Desenvolva sua soluÃ§Ã£o
3. Atualize o README com instruÃ§Ãµes de execuÃ§Ã£o
4. Envie o link do seu repositÃ³rio

## Estrutura da SoluÃ§Ã£o V2

### ğŸ—ï¸ Arquitetura Implementada
```
Python (ExtraÃ§Ã£o) â†’ PySpark (ConsolidaÃ§Ã£o) â†’ SQL (AnÃ¡lises)
```

### 1. ExtraÃ§Ã£o Python (src/)
- Download robusto via requests
- Upload para DBFS
- ValidaÃ§Ã£o de integridade

### 2. ConsolidaÃ§Ã£o PySpark (src/)
- **Raw Layer**: Dados brutos
- **Bronze Layer**: Padronizados e limpos
- **Silver Layer**: Enriquecidos e validados
- **Gold Layer**: Agregados para anÃ¡lises

### 3. AnÃ¡lises SQL (sql/)
- Consultas otimizadas na camada Gold
- Respostas Ã s perguntas do case
- Insights de negÃ³cio

## Tecnologias Utilizadas V2

- **Python**: ExtraÃ§Ã£o de dados (requests, os, time)
- **PySpark**: ConsolidaÃ§Ã£o em 4 camadas Delta Lake
- **SQL**: AnÃ¡lises finais otimizadas
- **Delta Lake**: Armazenamento ACID com particionamento
- **Databricks Community Edition**: Ambiente de execuÃ§Ã£o

## Como Executar

### PrÃ©-requisitos
- Conta no Databricks Community Edition (https://community.cloud.databricks.com/)
- Acesso Ã  internet para download dos dados
- Cluster Databricks com runtime 13.3 LTS ou superior

### OpÃ§Ã£o 1: ExecuÃ§Ã£o via Notebooks V2 (Recomendado)

1. **Clone este repositÃ³rio**
   ```bash
   git clone <seu-repositorio>
   ```

2. **Importe os notebooks V2 no Databricks**
   - Acesse seu workspace Databricks
   - VÃ¡ em "Workspace" â†’ "Import"
   - FaÃ§a upload dos arquivos da pasta `notebooks/`
   - Ou importe diretamente do GitHub

3. **Execute os notebooks V2 na ordem:**
   - `01_Data_Extraction_Python.py` - ExtraÃ§Ã£o via Python
   - `02_Data_Consolidation_PySpark.py` - ConsolidaÃ§Ã£o 4 camadas
   - `03_Business_Analysis_SQL.py` - AnÃ¡lises SQL finais

### OpÃ§Ã£o 2: ExecuÃ§Ã£o via Pipeline V2

1. **Configure o ambiente**
   ```bash
   pip install -r requirements.txt
   ```

2. **Execute o pipeline completo V2**
   ```python
   # No Databricks
   %run src/main_pipeline_v2.py
   ```

3. **Execute consultas SQL**
   ```sql
   -- Carregue e execute
   %run sql/business_questions.sql
   ```

### Estrutura de ExecuÃ§Ã£o

```
1. ExtraÃ§Ã£o Python (src/data_extraction.py)
   â”œâ”€â”€ Download via requests
   â”œâ”€â”€ Upload para DBFS
   â”œâ”€â”€ ValidaÃ§Ã£o de integridade
   â””â”€â”€ PreparaÃ§Ã£o para PySpark

2. ConsolidaÃ§Ã£o PySpark (src/data_consolidation.py)
   â”œâ”€â”€ Raw Layer (dados brutos)
   â”œâ”€â”€ Bronze Layer (padronizados)
   â”œâ”€â”€ Silver Layer (enriquecidos)
   â”œâ”€â”€ Gold Layer (agregados)
   â””â”€â”€ OtimizaÃ§Ã£o Delta Lake

3. AnÃ¡lises SQL (sql/business_questions.sql)
   â”œâ”€â”€ Pergunta 1: MÃ©dia Yellow Taxis
   â”œâ”€â”€ Pergunta 2: Passageiros por hora em Maio
   â”œâ”€â”€ AnÃ¡lises complementares
   â””â”€â”€ Insights de negÃ³cio
```

