"""
Pipeline principal V2 - Arquitetura Python + PySpark + SQL
Orquestra: Extra√ß√£o Python ‚Üí Consolida√ß√£o PySpark ‚Üí An√°lises SQL
"""

import time
from pyspark.sql import SparkSession

from data_extraction import NYCTaxiDataExtractor
from data_consolidation import DataLakeConsolidator
from config import get_table_name


class NYCTaxiPipelineV2:
    """
    Pipeline principal V2 com arquitetura em camadas
    """
    
    def __init__(self):
        """Inicializa o pipeline"""
        self.spark = SparkSession.builder \
            .appName("NYC_Taxi_Pipeline_V2") \
            .getOrCreate()
        
        self.start_time = time.time()
        self.extraction_results = None
        self.consolidation_success = False
        
    def run_complete_pipeline(self) -> bool:
        """
        Executa o pipeline completo V2
        
        Returns:
            True se executou com sucesso
        """
        print("üöÄ INICIANDO PIPELINE V2 - NYC TAXI DATA")
        print("Arquitetura: Python ‚Üí PySpark ‚Üí SQL")
        print("="*60)
        
        try:
            # Etapa 1: Extra√ß√£o via Python
            print("\nüì• ETAPA 1: EXTRA√á√ÉO DE DADOS (PYTHON)")
            print("-" * 50)
            
            extractor = NYCTaxiDataExtractor()
            self.extraction_results = extractor.extract_all_data()
            
            if self.extraction_results['success_count'] == 0:
                print("‚ùå Falha na extra√ß√£o dos dados. Pipeline interrompido.")
                return False
            
            print(f"‚úÖ Extra√ß√£o conclu√≠da: {self.extraction_results['success_count']} arquivos")
            
            # Etapa 2: Consolida√ß√£o via PySpark
            print("\nüîÑ ETAPA 2: CONSOLIDA√á√ÉO DATA LAKE (PYSPARK)")
            print("-" * 50)
            
            consolidator = DataLakeConsolidator(self.spark)
            self.consolidation_success = consolidator.run_full_consolidation(
                self.extraction_results['successful_files']
            )
            
            if not self.consolidation_success:
                print("‚ùå Falha na consolida√ß√£o. Pipeline interrompido.")
                return False
            
            print("‚úÖ Consolida√ß√£o conclu√≠da: Camadas Raw ‚Üí Bronze ‚Üí Silver ‚Üí Gold")
            
            # Etapa 3: Prepara√ß√£o para an√°lises SQL
            print("\nüìä ETAPA 3: PREPARA√á√ÉO PARA AN√ÅLISES (SQL)")
            print("-" * 50)
            
            self.prepare_sql_analysis()
            
            # Etapa 4: Relat√≥rio final
            print("\nüìã ETAPA 4: RELAT√ìRIO FINAL")
            print("-" * 50)
            
            self.generate_final_report()
            
            return True
            
        except Exception as e:
            print(f"üí• Erro cr√≠tico no pipeline: {e}")
            return False
        
        finally:
            self.cleanup()
    
    def prepare_sql_analysis(self):
        """
        Prepara ambiente para an√°lises SQL
        """
        try:
            print("üîß Preparando ambiente para an√°lises SQL...")
            
            # Verifica se todas as tabelas foram criadas
            tables_to_check = ["raw", "bronze", "silver", "gold"]
            available_tables = []
            
            for layer in tables_to_check:
                table_name = get_table_name(layer)
                try:
                    count = self.spark.table(table_name).count()
                    available_tables.append(layer)
                    print(f"  ‚úÖ {table_name}: {count:,} registros")
                except Exception as e:
                    print(f"  ‚ùå {table_name}: N√£o dispon√≠vel ({e})")
            
            if "gold" in available_tables:
                print("‚úÖ Camada Gold dispon√≠vel - Pronto para an√°lises SQL!")
                
                # Mostra exemplo de consulta
                print("\nüìù Exemplo de consulta SQL:")
                print("```sql")
                print("SELECT taxi_type, year_month, ")
                print("       SUM(total_trips) as viagens,")
                print("       ROUND(SUM(sum_total_amount)/SUM(total_trips), 2) as tarifa_media")
                print("FROM main.nyc_taxi.gold_trips")
                print("GROUP BY taxi_type, year_month")
                print("ORDER BY taxi_type, year_month;")
                print("```")
                
            else:
                print("‚ö†Ô∏è  Camada Gold n√£o dispon√≠vel - An√°lises limitadas")
            
        except Exception as e:
            print(f"‚ùå Erro na prepara√ß√£o SQL: {e}")
    
    def run_sql_business_questions(self):
        """
        Executa as perguntas de neg√≥cio em SQL
        """
        try:
            print("\nüìä EXECUTANDO PERGUNTAS DE NEG√ìCIO (SQL)")
            print("="*50)
            
            gold_table = get_table_name("gold")
            
            # Pergunta 1: M√©dia Yellow Taxis
            print("\nüéØ PERGUNTA 1: M√©dia de valor total Yellow Taxis")
            print("-" * 50)
            
            query1 = f"""
            SELECT 
                ROUND(SUM(sum_total_amount) / SUM(total_trips), 2) as media_valor_total,
                SUM(total_trips) as total_viagens,
                ROUND(SUM(sum_total_amount), 2) as receita_total
            FROM {gold_table}
            WHERE taxi_type = 'yellow'
            """
            
            result1 = self.spark.sql(query1)
            print("üí∞ RESPOSTA:")
            result1.show(truncate=False)
            
            # Detalhamento por m√™s
            query1_detail = f"""
            SELECT 
                year_month,
                ROUND(AVG(avg_total_amount), 2) as media_valor,
                SUM(total_trips) as viagens
            FROM {gold_table}
            WHERE taxi_type = 'yellow'
            GROUP BY year_month
            ORDER BY year_month
            """
            
            result1_detail = self.spark.sql(query1_detail)
            print("\nüìÖ Detalhamento por m√™s:")
            result1_detail.show(truncate=False)
            
            # Pergunta 2: Passageiros por hora em Maio
            print("\nüéØ PERGUNTA 2: Passageiros por hora em Maio")
            print("-" * 50)
            
            query2 = f"""
            SELECT 
                pickup_hour as hora,
                ROUND(SUM(sum_passenger_count) / SUM(total_trips), 2) as media_passageiros,
                SUM(total_trips) as viagens
            FROM {gold_table}
            WHERE pickup_month = 5 AND pickup_year = 2023
            GROUP BY pickup_hour
            ORDER BY pickup_hour
            """
            
            result2 = self.spark.sql(query2)
            print("üë• RESPOSTA:")
            result2.show(24, truncate=False)
            
            # Identifica picos
            query2_peaks = f"""
            SELECT 
                pickup_hour as hora,
                ROUND(SUM(sum_passenger_count) / SUM(total_trips), 2) as media_passageiros,
                SUM(total_trips) as viagens
            FROM {gold_table}
            WHERE pickup_month = 5 AND pickup_year = 2023
            GROUP BY pickup_hour
            ORDER BY media_passageiros DESC
            LIMIT 3
            """
            
            result2_peaks = self.spark.sql(query2_peaks)
            print("\nüî• Top 3 horas com mais passageiros:")
            result2_peaks.show(truncate=False)
            
        except Exception as e:
            print(f"‚ùå Erro nas consultas SQL: {e}")
    
    def generate_final_report(self):
        """
        Gera relat√≥rio final do pipeline V2
        """
        end_time = time.time()
        duration = end_time - self.start_time
        
        print("\n" + "="*60)
        print("üìã RELAT√ìRIO FINAL DO PIPELINE V2")
        print("="*60)
        
        # Informa√ß√µes de tempo
        print(f"\n‚è±Ô∏è  Tempo de Execu√ß√£o:")
        print(f"   ‚Ä¢ Dura√ß√£o total: {duration:.2f} segundos ({duration/60:.2f} minutos)")
        
        # Informa√ß√µes de extra√ß√£o
        if self.extraction_results:
            print(f"\nüì• Extra√ß√£o de Dados (Python):")
            print(f"   ‚Ä¢ Arquivos extra√≠dos: {self.extraction_results['success_count']}")
            print(f"   ‚Ä¢ Arquivos com falha: {self.extraction_results['failure_count']}")
            print(f"   ‚Ä¢ Taxa de sucesso: {self.extraction_results['success_count']/(self.extraction_results['success_count']+self.extraction_results['failure_count'])*100:.1f}%")
        
        # Informa√ß√µes de consolida√ß√£o
        print(f"\nüîÑ Consolida√ß√£o Data Lake (PySpark):")
        print(f"   ‚Ä¢ Status: {'‚úÖ Sucesso' if self.consolidation_success else '‚ùå Falha'}")
        
        if self.consolidation_success:
            try:
                # Conta registros em cada camada
                layers = ["raw", "bronze", "silver", "gold"]
                print(f"   ‚Ä¢ Camadas criadas:")
                
                for layer in layers:
                    table_name = get_table_name(layer)
                    try:
                        count = self.spark.table(table_name).count()
                        print(f"     - {layer.upper()}: {count:,} registros")
                    except:
                        print(f"     - {layer.upper()}: Erro ao contar")
                        
            except Exception as e:
                print(f"   ‚Ä¢ Erro ao gerar estat√≠sticas: {e}")
        
        # Informa√ß√µes de an√°lises
        print(f"\nüìä An√°lises SQL:")
        print(f"   ‚Ä¢ Arquivo SQL: sql/business_questions.sql")
        print(f"   ‚Ä¢ Tabela principal: {get_table_name('gold')}")
        print(f"   ‚Ä¢ Perguntas implementadas: 2 principais + an√°lises complementares")
        
        # Arquitetura implementada
        print(f"\nüèóÔ∏è  Arquitetura Implementada:")
        print(f"   ‚Ä¢ Extra√ß√£o: Python (requests + DBFS)")
        print(f"   ‚Ä¢ Processamento: PySpark (4 camadas)")
        print(f"   ‚Ä¢ An√°lises: SQL (consultas otimizadas)")
        print(f"   ‚Ä¢ Armazenamento: Delta Lake (particionado)")
        
        # Pr√≥ximos passos
        print(f"\nüéØ Pr√≥ximos Passos:")
        print(f"   1. Execute: %run sql/business_questions.sql")
        print(f"   2. Explore: SELECT * FROM {get_table_name('gold')} LIMIT 10")
        print(f"   3. Analise: Consultas personalizadas na camada Gold")
        print(f"   4. Visualize: Crie dashboards com os dados")
        
        print(f"\n‚úÖ Pipeline V2 executado com sucesso!")
        print("="*60)
    
    def cleanup(self):
        """Limpeza final do pipeline"""
        try:
            # Limpa cache do Spark
            self.spark.catalog.clearCache()
            print("üßπ Cache do Spark limpo")
        except Exception as e:
            print(f"‚ö†Ô∏è  Erro na limpeza: {e}")


def main():
    """Fun√ß√£o principal para execu√ß√£o do pipeline V2"""
    pipeline = NYCTaxiPipelineV2()
    success = pipeline.run_complete_pipeline()
    
    if success:
        print("\nüéâ Pipeline V2 conclu√≠do com sucesso!")
        
        # Executa an√°lises SQL se solicitado
        try:
            pipeline.run_sql_business_questions()
        except Exception as e:
            print(f"‚ö†Ô∏è  An√°lises SQL n√£o executadas: {e}")
            print("üí° Execute manualmente: %run sql/business_questions.sql")
        
        return 0
    else:
        print("\nüí• Pipeline V2 falhou!")
        return 1


if __name__ == "__main__":
    exit(main())
