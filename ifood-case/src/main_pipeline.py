"""
Pipeline principal para execu√ß√£o completa do projeto
Orquestra ingest√£o, valida√ß√£o e prepara√ß√£o dos dados
"""

import time
from pyspark.sql import SparkSession

from data_ingestion import NYCTaxiDataIngestion
from data_quality import DataQualityValidator, validate_table_quality
from config import get_full_table_name


class NYCTaxiPipeline:
    """
    Pipeline principal para processamento completo dos dados de t√°xi NYC
    """
    
    def __init__(self):
        """Inicializa o pipeline"""
        self.spark = SparkSession.builder \
            .appName("NYC_Taxi_Complete_Pipeline") \
            .getOrCreate()
        
        self.start_time = time.time()
        
    def run_complete_pipeline(self) -> bool:
        """
        Executa o pipeline completo
        
        Returns:
            True se executou com sucesso
        """
        print("üöÄ INICIANDO PIPELINE COMPLETO - NYC TAXI DATA")
        print("="*60)
        
        try:
            # Etapa 1: Ingest√£o dos dados
            print("\nüì• ETAPA 1: INGEST√ÉO DOS DADOS")
            print("-" * 40)
            
            ingestion = NYCTaxiDataIngestion(self.spark)
            ingestion_success = ingestion.process_all_data()
            
            if not ingestion_success:
                print("‚ùå Falha na ingest√£o dos dados. Pipeline interrompido.")
                return False
            
            # Etapa 2: Valida√ß√£o de qualidade
            print("\nüîç ETAPA 2: VALIDA√á√ÉO DE QUALIDADE")
            print("-" * 40)
            
            validation_results = validate_table_quality(self.spark)
            
            if "error" in validation_results:
                print("‚ö†Ô∏è  Problemas na valida√ß√£o, mas continuando...")
            
            # Etapa 3: Otimiza√ß√£o da tabela
            print("\n‚ö° ETAPA 3: OTIMIZA√á√ÉO DA TABELA")
            print("-" * 40)
            
            optimization_success = self.optimize_table()
            
            # Etapa 4: Relat√≥rio final
            print("\nüìä ETAPA 4: RELAT√ìRIO FINAL")
            print("-" * 40)
            
            self.generate_final_report(ingestion, validation_results)
            
            return True
            
        except Exception as e:
            print(f"üí• Erro cr√≠tico no pipeline: {e}")
            return False
        
        finally:
            self.cleanup()
    
    def optimize_table(self) -> bool:
        """
        Otimiza a tabela Delta criada
        
        Returns:
            True se otimizou com sucesso
        """
        try:
            table_name = get_full_table_name()
            
            print(f"üîß Otimizando tabela {table_name}...")
            
            # OPTIMIZE para compacta√ß√£o de arquivos
            self.spark.sql(f"OPTIMIZE {table_name}")
            print("‚úÖ Compacta√ß√£o de arquivos conclu√≠da")
            
            # Z-ORDER para otimizar consultas por datetime e taxi_type
            try:
                self.spark.sql(f"OPTIMIZE {table_name} ZORDER BY (tpep_pickup_datetime, taxi_type)")
                print("‚úÖ Z-ORDER aplicado para otimizar consultas")
            except Exception as e:
                print(f"‚ö†Ô∏è  Z-ORDER n√£o aplicado: {e}")
            
            # ANALYZE TABLE para estat√≠sticas
            try:
                self.spark.sql(f"ANALYZE TABLE {table_name} COMPUTE STATISTICS")
                print("‚úÖ Estat√≠sticas da tabela atualizadas")
            except Exception as e:
                print(f"‚ö†Ô∏è  Estat√≠sticas n√£o atualizadas: {e}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Erro na otimiza√ß√£o: {e}")
            return False
    
    def generate_final_report(self, ingestion: NYCTaxiDataIngestion, validation_results: dict):
        """
        Gera relat√≥rio final do pipeline
        
        Args:
            ingestion: Inst√¢ncia da classe de ingest√£o
            validation_results: Resultados da valida√ß√£o
        """
        end_time = time.time()
        duration = end_time - self.start_time
        
        print("\n" + "="*60)
        print("üìã RELAT√ìRIO FINAL DO PIPELINE")
        print("="*60)
        
        # Informa√ß√µes de tempo
        print(f"\n‚è±Ô∏è  Tempo de Execu√ß√£o:")
        print(f"   ‚Ä¢ Dura√ß√£o total: {duration:.2f} segundos ({duration/60:.2f} minutos)")
        
        # Informa√ß√µes de ingest√£o
        print(f"\nüì• Ingest√£o de Dados:")
        print(f"   ‚Ä¢ Arquivos processados com sucesso: {ingestion.successful_loads}")
        print(f"   ‚Ä¢ Arquivos com falha: {ingestion.failed_loads}")
        print(f"   ‚Ä¢ Total de registros ingeridos: {ingestion.total_records:,}")
        
        # Informa√ß√µes de qualidade
        if "quality_summary" in validation_results:
            quality = validation_results["quality_summary"]
            print(f"\nüîç Qualidade dos Dados:")
            print(f"   ‚Ä¢ Status geral: {quality['overall_status']}")
            print(f"   ‚Ä¢ Issues identificados: {quality['total_issues']}")
        
        # Informa√ß√µes da tabela final
        try:
            table_name = get_full_table_name()
            df = self.spark.table(table_name)
            final_count = df.count()
            
            print(f"\nüìä Tabela Final:")
            print(f"   ‚Ä¢ Nome: {table_name}")
            print(f"   ‚Ä¢ Total de registros: {final_count:,}")
            print(f"   ‚Ä¢ Colunas: {len(df.columns)}")
            
            # Distribui√ß√£o por tipo e m√™s
            print(f"\nüìà Distribui√ß√£o dos Dados:")
            print("   Por tipo de t√°xi:")
            type_dist = df.groupBy("taxi_type").count().collect()
            for row in type_dist:
                print(f"     - {row['taxi_type']}: {row['count']:,} registros")
            
            print("   Por m√™s:")
            month_dist = df.groupBy("year_month").count().orderBy("year_month").collect()
            for row in month_dist:
                print(f"     - {row['year_month']}: {row['count']:,} registros")
                
        except Exception as e:
            print(f"‚ö†Ô∏è  Erro ao gerar estat√≠sticas finais: {e}")
        
        # Pr√≥ximos passos
        print(f"\nüéØ Pr√≥ximos Passos:")
        print(f"   1. Execute as an√°lises em 'analysis/business_questions.py'")
        print(f"   2. Explore os dados com 'analysis/exploratory_analysis.py'")
        print(f"   3. Consulte a tabela: {get_full_table_name()}")
        
        print(f"\n‚úÖ Pipeline executado com sucesso!")
        print("="*60)
    
    def cleanup(self):
        """Limpeza final do pipeline"""
        try:
            # Limpa cache do Spark
            self.spark.catalog.clearCache()
            print("üßπ Cache do Spark limpo")
        except Exception as e:
            print(f"‚ö†Ô∏è  Erro na limpeza: {e}")


def main():
    """Fun√ß√£o principal para execu√ß√£o do pipeline"""
    pipeline = NYCTaxiPipeline()
    success = pipeline.run_complete_pipeline()
    
    if success:
        print("\nüéâ Pipeline conclu√≠do com sucesso!")
        return 0
    else:
        print("\nüí• Pipeline falhou!")
        return 1


if __name__ == "__main__":
    exit(main())
